{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a text leaning on previous n words. Nothing about ML, just math, ya know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>day</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>month</th>\n",
       "      <th>summary</th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12519</th>\n",
       "      <td>[{'name': 'Yi Zhou'}, {'name': 'Yingbin Liang'...</td>\n",
       "      <td>19</td>\n",
       "      <td>1802.06903v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>2</td>\n",
       "      <td>The success of deep learning has led to a risi...</td>\n",
       "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
       "      <td>Generalization Error Bounds with Probabilistic...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7801</th>\n",
       "      <td>[{'name': 'Sean Billings'}]</td>\n",
       "      <td>12</td>\n",
       "      <td>1803.04489v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>3</td>\n",
       "      <td>This paper explores the recently proposed Grap...</td>\n",
       "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Probabilistic and Regularized Graph Convolutio...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33368</th>\n",
       "      <td>[{'name': 'Susan Khor'}]</td>\n",
       "      <td>18</td>\n",
       "      <td>1107.3498v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>7</td>\n",
       "      <td>Slow self-avoiding adaptive walks by an infini...</td>\n",
       "      <td>[{'term': 'cs.NE', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>What can we learn from slow self-avoiding adap...</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36574</th>\n",
       "      <td>[{'name': 'Alexandru Baltag'}, {'name': 'Bryan...</td>\n",
       "      <td>27</td>\n",
       "      <td>1503.08141v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>3</td>\n",
       "      <td>The theory $\\mathsf{CDL}$ of Conditional Doxas...</td>\n",
       "      <td>[{'term': 'cs.LO', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Revisable Justified Belief: Preliminary Report</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17409</th>\n",
       "      <td>[{'name': 'Bernardt Duvenhage'}, {'name': 'Mfu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1711.00247v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>11</td>\n",
       "      <td>Virtual assistants and text chatbots have rece...</td>\n",
       "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Improved Text Language Identification for the ...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  author  day            id  \\\n",
       "12519  [{'name': 'Yi Zhou'}, {'name': 'Yingbin Liang'...   19  1802.06903v1   \n",
       "7801                         [{'name': 'Sean Billings'}]   12  1803.04489v1   \n",
       "33368                           [{'name': 'Susan Khor'}]   18   1107.3498v1   \n",
       "36574  [{'name': 'Alexandru Baltag'}, {'name': 'Bryan...   27  1503.08141v1   \n",
       "17409  [{'name': 'Bernardt Duvenhage'}, {'name': 'Mfu...    1  1711.00247v1   \n",
       "\n",
       "                                                    link  month  \\\n",
       "12519  [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
       "7801   [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n",
       "33368  [{'rel': 'alternate', 'href': 'http://arxiv.or...      7   \n",
       "36574  [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n",
       "17409  [{'rel': 'alternate', 'href': 'http://arxiv.or...     11   \n",
       "\n",
       "                                                 summary  \\\n",
       "12519  The success of deep learning has led to a risi...   \n",
       "7801   This paper explores the recently proposed Grap...   \n",
       "33368  Slow self-avoiding adaptive walks by an infini...   \n",
       "36574  The theory $\\mathsf{CDL}$ of Conditional Doxas...   \n",
       "17409  Virtual assistants and text chatbots have rece...   \n",
       "\n",
       "                                                     tag  \\\n",
       "12519  [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
       "7801   [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
       "33368  [{'term': 'cs.NE', 'scheme': 'http://arxiv.org...   \n",
       "36574  [{'term': 'cs.LO', 'scheme': 'http://arxiv.org...   \n",
       "17409  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
       "\n",
       "                                                   title  year  \n",
       "12519  Generalization Error Bounds with Probabilistic...  2018  \n",
       "7801   Probabilistic and Regularized Graph Convolutio...  2018  \n",
       "33368  What can we learn from slow self-avoiding adap...  2011  \n",
       "36574     Revisable Justified Belief: Preliminary Report  2015  \n",
       "17409  Improved Text Language Identification for the ...  2017  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(\"./arxivData.json\")\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
       " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
       " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'].replace(\"\\n\", ' '), axis=1).tolist()\n",
    "\n",
    "sorted(lines, key=len)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "lines = [\" \".join(tokenizer.tokenize(line)).lower() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'differential contrastive divergence ; this paper has been retracted .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lines, key=len)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted(lines, key=len)[0] == \\\n",
    "    'differential contrastive divergence ; this paper has been retracted .'\n",
    "assert sorted(lines, key=len)[2] == \\\n",
    "    'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Tuple, LiteralString, List\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
    "\n",
    "def count_ngrams(lines, n) -> dict:\n",
    "    res = defaultdict(Counter)\n",
    "\n",
    "    for line in tqdm(lines, desc='Total'):\n",
    "        corrected_line = (n - 1) * [UNK] + line.split() + [EOS]\n",
    "        for word_ind in range(n - 1, len(corrected_line)):\n",
    "            key = tuple(corrected_line[word_ind-n + 1:word_ind])\n",
    "            res[key][corrected_line[word_ind]] += 1\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: 100%|██████████| 100/100 [00:00<00:00, 16671.19it/s]\n"
     ]
    }
   ],
   "source": [
    "dummy_lines = sorted(lines, key=len)[:100]\n",
    "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
    "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
    "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
    "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
    "assert dummy_counts['p', '=']['np'] == 2\n",
    "assert dummy_counts['author', '.']['_EOS_'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:    \n",
    "    def __init__(self, lines, n):\n",
    "        \"\"\" \n",
    "        Train a simple count-based language model: \n",
    "        compute probabilities P(w_t | prefix) given ngram counts\n",
    "        \n",
    "        :param n: computes probability of next token given (n - 1) previous words\n",
    "        :param lines: an iterable of strings with space-separated tokens\n",
    "        \"\"\"\n",
    "        assert n >= 1\n",
    "        self.n = n\n",
    "    \n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        \n",
    "        # compute token proabilities given counts\n",
    "        self.probs = defaultdict(Counter)\n",
    "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
    "        \n",
    "        # populate self.probs with actual probabilities\n",
    "        \n",
    "        for key in counts.keys():\n",
    "            for value in counts[key]:\n",
    "                self.probs[key][value] = counts[key][value] / counts[key].total()\n",
    "        \n",
    "            \n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
    "        \"\"\"\n",
    "        prefix = prefix.split()\n",
    "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
    "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
    "        return self.probs[tuple(prefix)]\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :param next_token: the next token to predict probability for\n",
    "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
    "        \"\"\"\n",
    "        return self.get_possible_next_tokens(prefix).get(next_token, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: 100%|██████████| 100/100 [00:00<00:00, 24985.43it/s]\n"
     ]
    }
   ],
   "source": [
    "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
    "\n",
    "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
    "assert np.allclose(p_initial['learning'], 0.02)\n",
    "assert np.allclose(p_initial['a'], 0.13)\n",
    "assert np.allclose(p_initial.get('meow', 0), 0)\n",
    "assert np.allclose(sum(p_initial.values()), 1)\n",
    "\n",
    "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
    "assert np.allclose(p_a['machine'], 0.15384615)\n",
    "assert np.allclose(p_a['note'], 0.23076923)\n",
    "assert np.allclose(p_a.get('the', 0), 0)\n",
    "assert np.allclose(sum(p_a.values()), 1)\n",
    "\n",
    "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
    "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
    "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
    "    \"your 3-gram model should only depend on 2 previous words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: 100%|██████████| 41000/41000 [00:11<00:00, 3533.23it/s]\n"
     ]
    }
   ],
   "source": [
    "lm = NGramLanguageModel(lines, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "\n",
    "def get_next_token(lm, prefix, temperature=1.0):\n",
    "    words = list(lm.get_possible_next_tokens(prefix).keys())\n",
    "\n",
    "    if temperature == 0:\n",
    "        probs = [lm.get_next_token_prob(prefix, word) for word in words]\n",
    "        return words[probs.index(max(probs))]\n",
    "\n",
    "    probs = [lm.get_next_token_prob(prefix, word) ** (1.0 / temperature) for word in words]\n",
    "\n",
    "    word = choices(words, weights=probs)[0]\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8643\n",
      "Looks nice!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
    "assert 250 < test_freqs['not'] < 450\n",
    "assert 8500 < test_freqs['been'] < 9500\n",
    "assert 1 < test_freqs['lately'] < 200\n",
    "\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
    "assert 1500 < test_freqs['learning'] < 3000\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
    "assert 8000 < test_freqs['learning'] < 9000\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
    "assert test_freqs['learning'] == 10000\n",
    "\n",
    "print(\"Looks nice!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial neural network architectures ( including radial distortion is then refined by gaussian regression ; we address these challenges by developing a pe . the performance and increase revenue , especially for those classes ? is there a difference of samples . and adversarial ). experimental results demonstrate the ability of handling newly created problem specific characteristics . this camera image processing . recently , much of the marginal distribution function , and this is achieved using random sample consensus ( ransac ) approach . _EOS_\n"
     ]
    }
   ],
   "source": [
    "prefix = 'artificial' # <- your ideas :)\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridging the gap between the distributions , and the analysis of the user to the large - scale data sets . _EOS_\n"
     ]
    }
   ],
   "source": [
    "prefix = 'bridging the' # <- more of your ideas\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
